env_name: !!str WallRoom
device: !!str cpu                           # torch device (cuda:0 or cpu)
render_env: !!bool False                    # render environment

agents:
  gtn:
    mode: !!str 'multi'                     # 'single': run on single PC / 'multi': run on multiple PCs
    max_iterations: !!int 50                # maximum number of GTN iterations
    num_threads_per_worker: !!int 1         # how many pytorch/OMP threads per worker
    num_workers: !!int 16                   # size of the population
    num_test_envs: !!int -1                 # number of different test environments (-1: genereate default test env)
    noise_std: !!float 1e-1                 # standard deviation of the noise vector
    step_size: !!float 1                    # reptile step size
    nes_step_size: !!bool False             # when set to true, divide step size by the number of workers
    mirrored_sampling: !!bool True          # use normal or mirrored sampling
    num_grad_evals: !!int 1                 # how often to evaluate each gradient
    grad_eval_type: !!str mean              # mean or min
    weight_decay: !!float 0.0               # weight decay
    exploration_gain: !!float 0             # additional gain factor to favor exploration
    time_mult: !!float 3                    # maximum allocated time as multiplicative of avg. time
    time_max: !!float 100                   # maximum allocated time for first iteration
    time_sleep_master: !!float 0.2          # cycle sleeping time when waiting for data
    time_sleep_worker: !!float 2            # cycle sleeping time when waiting for data
    minimize_score: !!bool False            # perform argmin or argmax
    score_transform_type: !!int 7           # 0-7
    quit_when_solved: !!bool True           # continue training once the environment has been solved?
    synthetic_env_type: !!int 0             # 0: reward env / 1: virtual env
    agent_name: !!str QL                    # which RL agent to use for meta-training

  ql:
    train_episodes: !!int 100               # maximum number of episodes to optimize
    test_episodes: !!int 10                 # maximum number of episodes to optimize
    init_episodes: !!int 5                  # number of episodes to prevent an early out
    alpha: !!float 0.5                      # Q-Learning update factor
    gamma: !!float 0.9                      # discount factor
    eps_init: !!float 0.5                   # initial random action percentage
    eps_min: !!float 0.01                   # final random action percentage
    eps_decay: !!float 0.95                 # random action decay factor
    action_noise: !!float 0.0               # random action decay factor
    action_noise_decay: !!float 0.0         # random action decay factor
    same_action_num: !!int 1                # how often to perform the same action subsequently
    print_rate: 1000                        # update rate of avg meters
    early_out_num: 10                       # based on how many training steps shall an early out happen
    early_out_virtual_diff: !!float 1e-2    # performance difference for an early out for virtual envs

  sarsa:
    train_episodes: !!int 100               # maximum number of episodes to optimize
    test_episodes: !!int 10                 # maximum number of episodes to optimize
    init_episodes: !!int 5                  # number of episodes to prevent an early out
    alpha: !!float 0.5                      # Q-Learning update factor
    gamma: !!float 0.9                      # discount factor
    eps_init: !!float 0.5                   # initial random action percentage
    eps_min: !!float 0.01                   # final random action percentage
    eps_decay: !!float 0.95                 # random action decay factor
    action_noise: !!float 0.0               # random action decay factor
    action_noise_decay: !!float 0.0         # random action decay factor
    same_action_num: !!int 1                # how often to perform the same action subsequently
    print_rate: 1000                        # update rate of avg meters
    early_out_num: 10                       # based on how many training steps shall an early out happen
    early_out_virtual_diff: !!float 1e-2    # performance difference for an early out for virtual envs

envs:
  EmptyRoom22:
    solved_reward: !!float 0.8
    max_steps: !!int 10
    activation_fn: !!str leakyrelu          # activation function of the virtual environment
    hidden_size: 32                         # size of the hidden layer of the virtual environment
    hidden_layer: !!int 1                   # number of hidden layers of the virtual environment
    reward_env_type: !!int 0                # type of reward shaping function

  EmptyRoom23:
    solved_reward: !!float 0.8
    max_steps: !!int 15
    activation_fn: !!str leakyrelu          # activation function of the virtual environment
    hidden_size: 32                         # size of the hidden layer of the virtual environment
    hidden_layer: !!int 1                   # number of hidden layers of the virtual environment
    reward_env_type: !!int 3                # type of reward shaping function

  EmptyRoom33:
    solved_reward: !!float 0.8
    max_steps: !!int 20
    activation_fn: !!str leakyrelu          # activation function of the virtual environment
    hidden_size: 32                         # size of the hidden layer of the virtual environment
    hidden_layer: !!int 1                   # number of hidden layers of the virtual environment
    reward_env_type: !!int 3                # type of reward shaping function

  WallRoom:
    solved_reward: !!float 0.8
    max_steps: !!int 50
    activation_fn: !!str leakyrelu          # activation function of the virtual environment
    hidden_size: 32                         # size of the hidden layer of the virtual environment
    hidden_layer: !!int 1                   # number of hidden layers of the virtual environment
    reward_env_type: !!int 4                # type of reward shaping function

  HoleRoom:
    solved_reward: !!float 0.8
    max_steps: !!int 50
    activation_fn: !!str leakyrelu          # activation function of the virtual environment
    hidden_size: 32                         # size of the hidden layer of the virtual environment
    hidden_layer: !!int 1                   # number of hidden layers of the virtual environment
    reward_env_type: !!int 3                # type of reward shaping function
