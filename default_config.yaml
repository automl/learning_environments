env_name: !!str Pendulum-v0      # used gym environment like Pendulum-v0,
seed: !!int 0                    # random initialization seed

model:
  hidden_size: !!int 128         # size of the hidden layer
  activation_fn: !!str relu      # activation function for actor/critic ('tanh' or 'relu')
  action_std: !!float 0.1        # action noise standard deviation

td3:
  init_episodes: !!int 100       # number of episodes to fill the replay buffer
  max_episodes: !!int 100000     # maximum number of episodes to optimize
  batch_size: !!int  256         # batch size when running a policy update step
  gamma: !!float 0.99            # discount factor
  lr: !!float 2e-4               # learning rate
  tau: !!float 0.005             # target network update rate
  policy_delay: !!int 2          # frequency of delayed policy updates
  rb_size: !!int 1000000         # size of the replay buffer

ppo:
  max_episodes: !!int 100000     # maximum number of episodes to optimize
  update_episodes: !!float 5     # update policy every x episodes (can be float)
  ppo_epochs: !!int 10           # update policy for x epochs
  gamma: !!float 0.99            # discount factor
  lr: !!float 5e-4               # learning rate
  vf_coef: !!float 0.5           # value function coefficient (see PPO paper)
  ent_coef: !!float 0.1          # entropy coefficient (see PPO paper)
  eps_clip: !!float 0.2          # trust region size (see PPO paper)



