env_name: !!str Pendulum-v0        # used environment. Can be either a gym environment (e.g. Pendulum-v0) or a custom one (Pendulum)
seed: !!int 0                      # random initialization seed
render_env: !!bool True            # render environment
export_path: !!str "./exports/gtn_pendulum_td3/state.pth"

agents:
  reptile:
    max_iterations: !!int 10000    # maximum number of REPTILE iteration
    step_size: !!float 0.1         # step size when updating the model parameters
    agent_name: !!str TD3          # which RL agent to use for meta-training

  td3:
    init_episodes: !!int 10        # number of episodes to fill the replay buffer
    max_episodes: !!int 20         # maximum number of episodes to optimize
    batch_size: !!int  256         # batch size when running a policy update step
    gamma: !!float 0.99            # discount factor
    lr: !!float 3e-4               # learning rate
    tau: !!float 0.005             # target network update rate
    policy_delay: !!int 2          # frequency of delayed policy updates
    rb_size: !!int 1000000         # size of the replay buffer
    hidden_size: !!int 256         # size of the actor/critic hidden layer
    activation_fn: !!str relu      # activation function for actor/critic ('tanh' or 'relu')
    action_std: !!float 0.1        # action noise standard deviation
    weight_norm: !!bool True       # weight normalization of NN parameters of the learner

  ppo:
    max_episodes: !!int 100000     # maximum number of episodes to optimize
    update_episodes: !!float 20    # update policy every x episodes (can be float)
    ppo_epochs: !!int 100          # update policy for x epochs
    gamma: !!float 0.99            # discount factor
    lr: !!float 5e-4               # learning rate
    vf_coef: !!float 0.5           # value function coefficient (see PPO paper)
    ent_coef: !!float 0.01         # entropy coefficient (see PPO paper)
    eps_clip: !!float 0.2          # trust region size (see PPO paper)
    hidden_size: !!int 64          # size of the actor/critic hidden layer
    activation_fn: !!str relu      # activation function for actor/critic ('tanh' or 'relu')
    action_std: !!float 0.5        # action noise standard deviation
    weight_norm: !!bool True       # weight normalization of NN parameters of the learner

envs:
  Pendulum-v0:
    dt: [0.05, 0.05, 0.05]         # sampling rate
    max_steps: [10, 10, 10]        # maximum number of steps per episode
    max_speed: [10, 10, 10]        # maximum joint speed [min, default, max]
    max_torque: [0.5, 1, 2]        # maximum joint torque [min, default, max]
    g: [5, 10, 15]                 # gravitational constant [min, default, max]
    m: 1                           # joint mass [min, default, max]
    l: 1                           # joint length [min, default, max]
    hidden_size: 128               # size of the hidden layer of the corresponding virtual environment
    weight_norm: !!bool True       # weight normalization of NN parameters of the generator
    virtual_env_zero_init: !!bool False  # how should the virtual env be initialized? zero state or random starting state (i.e. according to gym specification)

#  PendulumEnv:
#    dt: [0.05, 0.05, 0.05]         # sampling rate
#    max_steps: [200, 200, 200]     # maximum number of steps per episode
#    max_speed: [5, 8, 10]          # maximum joint speed [min, default, max]
#    max_torque: [1, 2, 3]          # maximum joint torque [min, default, max]
#    g: [5, 10, 15]                 # gravitational constant [min, default, max]
#    m: [0.5, 1, 2]                 # joint mass [min, default, max]
#    l: [0.5, 1, 2]                 # joint length [min, default, max]
