env_name: !!str Pendulum-v0             # used environment. Can be either a gym environment (e.g. Pendulum-v0) or a custom one (Test)
seed: !!int 0                           # random initialization seed
render_env: !!bool False                # render environment
export_path: !!str "./exports/gtn_pendulum_td3/state.pth"

agents:
  gtn:
    max_iterations: !!int 100           # maximum number of GTN iterations
    match_iterations: !!int 1           # number of different environments the virtual env is matched to
    real_iterations: !!int 1            # number of iterations training the agent on the real env
    virtual_iterations: !!int 1         # number of iterations training the agent on the virtual env
    step_size: !!float 0.1              # reptile step size
    different_envs: !!int 20            # 0: use only default real env
    agent_name: !!str TD3               # which RL agent to use for meta-training

  reptile:
    max_iterations: !!int 10000         # maximum number of REPTILE iteration
    step_size: !!float 0.1              # step size when updating the model parameters
    agent_name: !!str PPO               # which RL agent to use for meta-training

  match_env:
    lr: !!float 1e-3                    # learning rate
    weight_decay: !!float 0             # weight decay
    batch_size: !!int 256               # batch size
    early_out_diff: !!float 0.001       # break criterion for matching
    early_out_num: !!int 50             # based on how many training steps shall an early out happen
    steps: !!int 50000                  # number of steps
    step_size: 500                      # LR scheduler step size
    gamma: 0.5                          # LR scheduler gamma

  td3:
    max_episodes: !!int 200             # maximum number of episodes to optimize
    init_episodes: !!int 100            # number of episodes to fill the replay buffer
    batch_size: !!int  256              # batch size when running a policy update step
    gamma: !!float 0.99                 # discount factor
    lr: !!float 3e-4                    # learning rate
    weight_decay: !!float 0             # weight decay
    tau: !!float 0.005                  # target network update rate
    policy_delay: !!int 2               # frequency of delayed policy updates
    rb_size: !!int 100000               # size of the replay buffer
    activation_fn: !!str leakyrelu      # activation function for actor/critic ('tanh', 'relu', 'leakyrelu' or 'prelu')
    hidden_size: !!int 224              # size of the actor/critic hidden layer
    hidden_layer: !!int 1               # number of hidden layers
    weight_norm: !!bool True            # use weight normalization
    action_std: !!float 0.1             # action noise standard deviation
    early_out_num: 5                    # based on how many training steps shall an early out happen
    optim_env_with_actor: !!bool True   # optimize env together with actor
    optim_env_with_critic: !!bool False # optimize env together with actor
    match_weight_actor: !!float 1e7     # how much to weight matching loss wrt actor loss
    match_weight_critic: !!float 1      # how much to weight matching loss wrt critic loss
    match_batch_size: !!int 256         # matching batch size

  ppo:
    max_episodes: !!int 100000          # maximum number of episodes to optimize
    update_episodes: !!float 20         # update policy every x episodes (can be float)
    ppo_epochs: !!int 100               # update policy for x epochs
    gamma: !!float 0.99                 # discount factor
    lr: !!float 5e-4                    # learning rate
    weight_decay: !!float 0             # weight decay
    vf_coef: !!float 0.5                # value function coefficient (see PPO paper)
    ent_coef: !!float 0.01              # entropy coefficient (see PPO paper)
    eps_clip: !!float 0.2               # trust region size (see PPO paper)
    activation_fn: !!str leakyrelu      # activation function for actor/critic ('tanh', 'relu', 'leakyrelu' or 'prelu')
    hidden_size: !!int 128              # size of the actor/critic hidden layer
    hidden_layer: !!int 1               # number of hidden layers
    weight_norm: !!bool False           # use weight normalization
    action_std: !!float 0.5             # action noise standard deviation
    early_out_num: 50                   # based on how many training steps shall an early out happen

envs:
  Pendulum-v0:
    solved_reward: !!float -300         # used for early out in RL agent training
    max_steps: [200, 200, 200]          # maximum number of steps per episode
    dt: [0.05, 0.05, 0.05]              # sampling rate
    max_speed: [8, 10, 12]              # maximum joint speed [min, default, max]
    max_torque: [0.5, 1, 2]             # maximum joint torque [min, default, max]
    g: [5, 10, 15]                      # gravitational constant [min, default, max]
    m: [0.5, 1, 1.5]                    # joint mass [min, default, max]
    l: [0.5, 1, 1.5]                    # joint length [min, default, max]
    activation_fn: !!str leakyrelu      # activation function for actor/critic ('tanh', 'relu', 'leakyrelu' or 'prelu')
    hidden_size: !!int 512              # size of the hidden layer of the corresponding virtual environment
    hidden_layer: !!int 1               # number of hidden layers
    zero_init: !!bool False             # use zero state or random starting state for initialization
    weight_norm: !!bool True            # use weight normalization

  Test:
    solved_reward: !!float 50           # used for early out in RL agent training
    max_steps: [200, 200, 200]          # maximum number of steps per episode
    activation_fn: !!str leakyrelu      # activation function for actor/critic ('tanh', 'relu', 'leakyrelu' or 'prelu')
    hidden_size: !!int 128              # size of the hidden layer of the corresponding virtual environment
    hidden_layer: !!int 1               # number of hidden layers
    zero_init: !!bool False             # use zero state or random starting state for initialization
    weight_norm: !!bool True            # use weight normalization for virtual environment

  MountainCarContinuous-v0:
    solved_reward: !!float 90           # used for early out in RL agent training
    max_steps: [200, 200, 200]          # maximum number of steps per episode
    activation_fn: !!str leakyrelu      # activation function for actor/critic ('tanh', 'relu', 'leakyrelu' or 'prelu')
    hidden_size: 128                    # size of the hidden layer of the corresponding virtual environment
    hidden_layer: !!int 1               # number of hidden layers
    zero_init: !!bool True              # use zero state or random starting state for initialization
    weight_norm: !!bool False           # use weight normalization for virtual environment


