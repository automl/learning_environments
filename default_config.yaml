#env_name: !!str Pendulum-v0
env_name: !!str CartPole-v0
seed: !!int 21                              # random initialization seed
device: !!str cuda:0                           # torch device (cuda:0 or cpu)
render_env: !!bool False                    # render environment

agents:
  gtn:
    mode: !!str 'single'                    # 'single': run on single PC / 'multi': run on multiple PCs
    max_iterations: !!int 100               # maximum number of GTN iterations
    num_threads_per_worker: !!int 1         # how many pytorch/OMP threads per worker
    num_workers: !!int 10                   # size of the population
    num_test_envs: !!int -1                 # number of different test environments (-1: genereate default test env)
    noise_std: !!float 1e-1                 # standard deviation of the noise vector
    step_size: !!float 1                    # reptile step size
    num_grad_evals: !!int 1                 # how often to evaluate each gradient
    weight_decay: !!float 0.0               # weight decay
    time_mult: !!float 3                    # maximum allocated time as multiplicative of avg. time
    time_max: !!float 100                   # maximum allocated time for first iteration
    time_sleep: !!float 0.001               # cycle sleeping time when waiting for data
    minimize_score: !!bool False            # perform argmin or argmax
    score_transform_type: !!int 4           # 0-4
    log_transf_zero_mean: !!bool False      # zero-mean log transform (->Wierstra2012) or worse half with zero weight
    log_transf_normalize: !!bool True       # normalize max log transform value to 1
    agent_name: !!str DDQN                  # which RL agent to use for meta-training

  reptile:
    max_iterations: !!int 20                # maximum number of REPTILE iteration
    step_size: !!float 0.1                  # step size when updating the model parameters
    agent_name: !!str TD3                   # which RL agent to use for meta-training
    parallel_update: !!bool True            # serial or parallel reptile updates?
    env_num: !!int 5                        # number of different training envs

  ddqn:
    # Bandit
#    train_episodes: !!int 1000              # maximum number of episodes to train
#    test_episodes: !!int 100                # maximum number of episodes to test
#    init_episodes: !!int 10                 # number of episodes to fill the replay buffer
#    batch_size: !!int 128                   # batch size when running a policy update step
#    gamma: !!float 0.98                     # discount factor
#    lr: !!float 5e-4                        # learning rate
#    tau: !!float 0.01                       # target network update rate
#    eps_init: !!float 0.1                   # initial random action percentage
#    eps_min: !!float 0.01                   # final random action percentage
#    eps_decay: !!float 0.8                  # random action decay factor
#    rb_size: !!int 1000000                  # size of the replay buffer
#    same_action_num: !!int 1                # how often to perform the same action subsequently
#    activation_fn: !!str relu               # activation function for actor/critic ('tanh', 'relu', 'leakyrelu' or 'prelu')
#    hidden_size: !!int 64                   # size of the actor/critic hidden layer
#    hidden_layer: !!int 1                   # number of hidden layers
#    print_rate: 10000                       # update rate of avg meters
#    early_out_num: 10                       # based on how many training steps shall an early out happen
#    early_out_virtual_diff: !!float 1e-2    # performance difference for an early out for virtual envs
    # CartPole
    train_episodes: !!int 2000              # maximum number of episodes to train
    test_episodes: !!int 100                # maximum number of episodes to test
    init_episodes: !!int 10                 # number of episodes to fill the replay buffer
    batch_size: !!int 128                   # batch size when running a policy update step
    gamma: !!float 0.99                     # discount factor
    lr: !!float 1e-3                        # learning rate
    tau: !!float 0.01                       # target network update rate
    eps_init: !!float 1                     # initial random action percentage
    eps_min: !!float 0.01                   # final random action percentage
    eps_decay: !!float 0.9                  # random action decay factor
    rb_size: !!int 1000                     # size of the replay buffer
    same_action_num: !!int 1                # how often to perform the same action subsequently
    activation_fn: !!str relu               # activation function for actor/critic ('tanh', 'relu', 'leakyrelu' or 'prelu')
    hidden_size: !!int 128                  # size of the actor/critic hidden layer
    hidden_layer: !!int 1                   # number of hidden layers
    print_rate: 10                        # update rate of avg meters
    early_out_num: 10                       # based on how many training steps shall an early out happen
    early_out_virtual_diff: !!float 1e-2    # performance difference for an early out for virtual envs
    # Gridworld
#    train_episodes: !!int 2000              # maximum number of episodes to train
#    test_episodes: !!int 100                # maximum number of episodes to test
#    init_episodes: !!int 10                 # number of episodes to fill the replay buffer
#    batch_size: !!int 128                   # batch size when running a policy update step
#    gamma: !!float 0.99                     # discount factor
#    lr: !!float 1e-3                        # learning rate
#    tau: !!float 0.01                       # target network update rate
#    eps_init: !!float 1                     # initial random action percentage
#    eps_min: !!float 0.01                   # final random action percentage
#    eps_decay: !!float 0.9                  # random action decay factor
#    rb_size: !!int 5000                     # size of the replay buffer
#    same_action_num: !!int 1                # how often to perform the same action subsequently
#    activation_fn: !!str relu               # activation function for actor/critic ('tanh', 'relu', 'leakyrelu' or 'prelu')
#    hidden_size: !!int 128                  # size of the actor/critic hidden layer
#    hidden_layer: !!int 1                   # number of hidden layers
#    print_rate: 100                         # update rate of avg meters
#    early_out_num: 10                       # based on how many training steps shall an early out happen
#    early_out_virtual_diff: !!float 1e-2    # performance difference for an early out for virtual envs

  td3:
    train_episodes: !!int 100               # maximum number of episodes to optimize
    test_episodes: !!int 10                 # maximum number of episodes to optimize
    init_episodes: !!int 25                 # number of episodes to fill the replay buffer
    batch_size: !!int  256                  # batch size when running a policy update step
    gamma: !!float 0.99                     # discount factor
    lr: !!float 1e-3                        # learning rate
    tau: !!float 0.02                       # target network update rate
    policy_delay: !!int 1                   # frequency of delayed policy updates
    rb_size: !!int 1000000                  # size of the replay buffer
    same_action_num: !!int 1                # how often to perform the same action subsequently
    activation_fn: !!str relu               # activation function for actor/critic ('tanh', 'relu', 'leakyrelu' or 'prelu')
    hidden_size: !!int 256                  # size of the actor/critic hidden layer
    hidden_layer: !!int 2                   # number of hidden layers
    action_std: !!float 0.1                 # action noise standard deviation
    policy_std: !!float 0.2                 # policy noise standard deviation
    policy_std_clip: !!float 0.5            # policy noise standard deviation
    print_rate: 1                           # update rate of avg meters
    early_out_num: !!int 5                  # based on how many training steps shall an early out happen
    early_out_virtual_diff: !!float 1e-2    # performance difference for an early out for virtual envs

  ppo:
    max_episodes: !!int 100000              # maximum number of episodes to optimize
    update_episodes: !!float 20             # update policy every x episodes (can be float)
    ppo_epochs: !!int 100                   # update policy for x epochs
    gamma: !!float 0.99                     # discount factor
    lr: !!float 5e-4                        # learning rate
    vf_coef: !!float 0.5                    # value function coefficient (see PPO paper)
    ent_coef: !!float 0.01                  # entropy coefficient (see PPO paper)
    eps_clip: !!float 0.2                   # trust region size (see PPO paper)
    same_action_num: !!int 1                # how often to perform the same action subsequently
    activation_fn: !!str relu               # activation function for actor/critic ('tanh', 'relu', 'leakyrelu' or 'prelu')
    hidden_size: !!int 128                  # size of the actor/critic hidden layer
    hidden_layer: !!int 1                   # number of hidden layers
    action_std: !!float 0.5                 # action noise standard deviation
    early_out_num: 50                       # based on how many training steps shall an early out happen

envs:
  Pendulum-v0:
    solved_reward: !!float -300             # used for early out in RL agent training
    max_steps: !!int 200                    # maximum number of steps per episode
    dt: !!float 0.05                        # sampling rate
    max_speed: [8, 10, 12, False]           # maximum joint speed [min, default, max, larger=more difficult]
    max_torque: [1.5, 2, 3, False]          # maximum joint torque [min, default, max, larger=more difficult]
    g: [5, 10, 15, True]                    # gravitational constant [min, default, max, larger=more difficult]
    m: [0.5, 1, 1.2, True]                  # joint mass [min, default, max, larger=more difficult]
    l: [0.5, 1, 1.2, True]                  # joint length [min, default, max, larger=more difficult]
    activation_fn: !!str leakyrelu          # activation function of the virtual environment
    hidden_size: 128                        # size of the hidden layer of the virtual environment
    hidden_layer: !!int 1                   # number of hidden layers of the virtual environment

  MountainCarContinuous-v0:
    solved_reward: !!float 80               # used for early out in RL agent training
    max_steps: !!int 999                    # maximum number of steps per episode REMOVE
    max_speed: [0.05, 0.07, 0.1, False]     # maximum allowed vehicle velocity
    power: [0.001, 0.0015, 0.002, False]    # strength of the vehicle engine
    goal_position: [0.45, 0.45, 0.5, True]  # x-value? of goal position
    goal_velocity: [0.0, 0.0, 0.05, True]   # minimum velocity when reaching the goal
    activation_fn: !!str leakyrelu          # activation function of the virtual environment
    hidden_size: 128                        # size of the hidden layer of the virtual environment
    hidden_layer: !!int 2                   # number of hidden layers of the virtual environment

  HalfCheetah-v2:
    solved_reward: !!float 10000            # used for early out in RL agent training
    max_steps: !!int 1000                   # maximum number of steps per episode
    g: [-0.2, -9.81, -50, True]             # gravitational constant along negative z-axis [min, default, max, larger=more difficult]
    cripple_joint: !!bool False             # randomly sample a joint to be disabled (the agent cannot apply torques to that joint)
    activation_fn: !!str leakyrelu          # activation function of the virtual environment
    hidden_size: 128                        # size of the hidden layer of the virtual environment
    hidden_layer: !!int 2                   # number of hidden layers of the virtual environment

  CartPole-v0:
    solved_reward: !!float 195              # used for early out in RL agent training
    max_steps: !!int 200                    # maximum number of steps per episode
    activation_fn: !!str leakyrelu          # activation function of the virtual environment
    hidden_size: 128                        # size of the hidden layer of the virtual environment
    hidden_layer: !!int 1                   # number of hidden layers of the virtual environment

  Bandit-v0:
    solved_reward: !!float 49
    max_steps: !!int 50
    activation_fn: !!str leakyrelu          # activation function of the virtual environment
    hidden_size: 4                          # size of the hidden layer of the virtual environment
    hidden_layer: !!int 1                   # number of hidden layers of the virtual environment

  LunarLander-v2:
    solved_reward: !!float 200
    max_steps: !!int 1000
    activation_fn: !!str leakyrelu          # activation function of the virtual environment
    hidden_size: 128                        # size of the hidden layer of the virtual environment
    hidden_layer: !!int 1                   # number of hidden layers of the virtual environment

  FrozenLake-v0:
    solved_reward: !!float 0.9
    max_steps: !!int 50
    activation_fn: !!str leakyrelu          # activation function of the virtual environment
    hidden_size: 16                         # size of the hidden layer of the virtual environment
    hidden_layer: !!int 1                   # number of hidden layers of the virtual environment

  Gridworld-v0:
    solved_reward: !!float 0.9
    max_steps: !!int 100
    activation_fn: !!str leakyrelu          # activation function of the virtual environment
    hidden_size: 16                         # size of the hidden layer of the virtual environment
    hidden_layer: !!int 1                   # number of hidden layers of the virtual environment

  CliffWalking-v0:
    solved_reward: !!float -50
    max_steps: !!int 100
    activation_fn: !!str leakyrelu          # activation function of the virtual environment
    hidden_size: 16                         # size of the hidden layer of the virtual environment
    hidden_layer: !!int 1                   # number of hidden layers of the virtual environment
