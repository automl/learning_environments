env_name: !!str Pendulum-v0             # used environment: Pendulum-v0, MountainCarCOntinuous-v0, Test
seed: !!int 0                           # random initialization seed
render_env: !!bool False                # render environment

agents:
  gtn:
    max_iterations: !!int 100           # maximum number of GTN iterations
    match_iterations: !!int 1           # number of different environments the virtual env is matched to
    real_prob: !!int 1                  # number of iterations training the agent on the real env
    virtual_prob: !!int 1               # number of iterations training the agent on the virtual env
    both_prob: !!int 1                  # number of iterations training the agent on the virtual env
    different_envs: !!int 10            # first env is default env
    agent_name: !!str TD3               # which RL agent to use for meta-training
    match_step_size: !!float 0.1        # reptile step size when matching the two environments
    real_step_size: !!float 0.1         # reptile step size when matching the agent to the real environment
    virtual_step_size: !!float 0.1      # reptile step size when matching the agent to the fixed virtual environment
    both_step_size: !!float 0.1         # reptile step size when matching the agent to the variable virtual env
    input_seed_mean: !!float 0.1        # mean of input seed distribution
    input_seed_range: !!float 0.1       # range (around mean) of input seed distribution

  reptile:
    max_iterations: !!int 10000         # maximum number of REPTILE iteration
    step_size: !!float 0.1              # step size when updating the model parameters
    agent_name: !!str TD3               # which RL agent to use for meta-training

  match_env:
    oversampling: !!float 1.5           # how much to sample beyond the normal state/action space
    lr: !!float 4e-3                    # learning rate
    weight_decay: !!float 1e-9          # weight decay
    batch_size: !!int 192               # batch size during training
    early_out_diff: !!float 1e-4        # break criterion for matching
    early_out_num: !!int 200            # based on how many training steps shall an early out happen
    steps: !!int 5000                   # number of steps
    step_size: !!int 500                # LR scheduler step size
    gamma: !!float 0.6                  # LR scheduler gamma

  td3:
    max_episodes: !!int 100             # maximum number of episodes to optimize
    init_episodes: !!int 50             # number of episodes to fill the replay buffer
    batch_size: !!int  256              # batch size when running a policy update step
    gamma: !!float 0.99                 # discount factor
    lr: !!float 1e-3                    # learning rate
    weight_decay: !!float 1e-9          # weight decay
    tau: !!float 0.02                   # target network update rate
    policy_delay: !!int 2               # frequency of delayed policy updates
    rb_size: !!int 1000000              # size of the replay buffer
    same_action_num: !!int 1            # how often to perform the same action subsequently
    activation_fn: !!str leakyrelu      # activation function for actor/critic ('tanh', 'relu', 'leakyrelu' or 'prelu')
    hidden_size: !!int 224              # size of the actor/critic hidden layer
    hidden_layer: !!int 1               # number of hidden layers
    weight_norm: !!bool False           # use weight normalization
    action_std: !!float 0.2             # action noise standard deviation
    early_out_num: 5                    # based on how many training steps shall an early out happen
    optim_env_with_actor: !!bool True   # optimize env together with actor
    optim_env_with_critic: !!bool False # optimize env together with critic
    match_weight_actor: !!float 1e4     # how much to weight matching loss wrt actor loss
    match_weight_critic: !!float 1e4    # how much to weight matching loss wrt critic loss
    match_batch_size: !!int 128         # matching batch size

  ppo:
    max_episodes: !!int 100000          # maximum number of episodes to optimize
    update_episodes: !!float 20         # update policy every x episodes (can be float)
    ppo_epochs: !!int 100               # update policy for x epochs
    gamma: !!float 0.99                 # discount factor
    lr: !!float 1e-3                    # learning rate
    weight_decay: !!float 0             # weight decay
    vf_coef: !!float 0.5                # value function coefficient (see PPO paper)
    ent_coef: !!float 0.01              # entropy coefficient (see PPO paper)
    eps_clip: !!float 0.2               # trust region size (see PPO paper)
    same_action_num: !!int 1            # how often to perform the same action subsequently
    activation_fn: !!str leakyrelu      # activation function for actor/critic ('tanh', 'relu', 'leakyrelu' or 'prelu')
    hidden_size: !!int 128              # size of the actor/critic hidden layer
    hidden_layer: !!int 1               # number of hidden layers
    weight_norm: !!bool False           # use weight normalization
    action_std: !!float 0.5             # action noise standard deviation
    early_out_num: 50                   # based on how many training steps shall an early out happen

envs:
  Pendulum-v0:
    solved_reward: !!float -300         # used for early out in RL agent training
    max_steps: 200                      # maximum number of steps per episode
    dt: 0.05                            # sampling rate
    max_speed: [8, 10, 12, False]       # maximum joint speed [min, default, max, larger=more difficult]
    max_torque: [1.5, 2, 3, False]      # maximum joint torque [min, default, max, larger=more difficult]
    g: [5, 10, 15, True]                # gravitational constant [min, default, max, larger=more difficult]
    m: [0.5, 1, 1.2, True]              # joint mass [min, default, max, larger=more difficult]
    l: [0.5, 1, 1.2, True]              # joint length [min, default, max, larger=more difficult]
    activation_fn: !!str tanh           # activation function for actor/critic ('tanh', 'relu', 'leakyrelu' or 'prelu')
    hidden_size: !!int 256              # size of the hidden layer of the corresponding virtual environment
    hidden_layer: !!int 2               # number of hidden layers
    zero_init: !!bool False             # use zero state or random starting state for initialization
    weight_norm: !!bool True            # use weight normalization

  MountainCarContinuous-v0:
    solved_reward: !!float 90           # used for early out in RL agent training
    max_steps: 999                      # maximum number of steps per episode
    max_speed: [0.05, 0.07, 0.1]        # maximum allowed vehicle velocity
    power: [0.001, 0.0015, 0.002]       # strength of the vehicle engine
    goal_position: [0.45, 0.45, 0.5]    # x value? of goal position
    activation_fn: !!str leakyrelu      # activation function for actor/critic ('tanh', 'relu', 'leakyrelu' or 'prelu')
    hidden_size: 128                    # size of the hidden layer of the corresponding virtual environment
    hidden_layer: !!int 1               # number of hidden layers
    zero_init: !!bool True              # use zero state or random starting state for initialization
    weight_norm: !!bool True            # use weight normalization for virtual environment

  Test:
    solved_reward: !!float 50           # used for early out in RL agent training
    max_steps: [200, 200, 200]          # maximum number of steps per episode
    activation_fn: !!str leakyrelu      # activation function for actor/critic ('tanh', 'relu', 'leakyrelu' or 'prelu')
    hidden_size: !!int 128              # size of the hidden layer of the corresponding virtual environment
    hidden_layer: !!int 1               # number of hidden layers
    zero_init: !!bool False             # use zero state or random starting state for initialization
    weight_norm: !!bool True            # use weight normalization for virtual environment


