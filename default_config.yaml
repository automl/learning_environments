env_name: !!str Pendulum-v0      # used environment. Can be either a gym environment (e.g. Pendulum-v0) or a custom one (Pendulum)
seed: !!int 0                    # random initialization seed

ac_model:
  hidden_size: !!int 128         # size of the hidden layer
  activation_fn: !!str relu      # activation function for actor/critic ('tanh' or 'relu')
  action_std: !!float 0.5        # action noise standard deviation

agents:
  td3:
    init_episodes: !!int 100       # number of episodes to fill the replay buffer
    max_episodes: !!int 100000     # maximum number of episodes to optimize
    batch_size: !!int  256         # batch size when running a policy update step
    gamma: !!float 0.99            # discount factor
    lr: !!float 2e-4               # learning rate
    tau: !!float 0.005             # target network update rate
    policy_delay: !!int 2          # frequency of delayed policy updates
    rb_size: !!int 1000000         # size of the replay buffer

  ppo:
    max_episodes: !!int 100000     # maximum number of episodes to optimize
    update_episodes: !!float 5     # update policy every x episodes (can be float)
    ppo_epochs: !!int 20           # update policy for x epochs
    gamma: !!float 0.99            # discount factor
    lr: !!float 5e-4               # learning rate
    vf_coef: !!float 0.5           # value function coefficient (see PPO paper)
    ent_coef: !!float 0.02         # entropy coefficient (see PPO paper)
    eps_clip: !!float 0.2          # trust region size (see PPO paper)

envs:
  PendulumEnv:
    max_speed: [5, 8, 10]        # maximum joint speed [min, default, max]
    max_torque: [1, 2, 3]        # maximum joint torque [min, default, max]
    g: [5, 10, 20]               # gravitational constant [min, default, max]
    m: [0.5, 1, 2]               # joint mass [min, default, max]
    l: [0.5, 1, 2]               # joint length [min, default, max]


